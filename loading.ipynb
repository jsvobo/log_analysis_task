{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import loading\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load selected logs for processing\n",
    "Some logs are not used, because they describe the zeek session and not the data.\n",
    "\n",
    "We decided to not use x509.log, since the data pertains to a very small portion of flows. This log captures details on certificates exchanged during certain TLS negotiations.\n",
    "https://docs.zeek.org/en/master/logs/x509.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conn.log...\n",
      "Loading dns.log...\n",
      "Loading files.log...\n",
      "Loading http.log...\n",
      "Loading ssl.log...\n",
      "\n",
      "dict_keys(['conn', 'dns', 'files', 'http', 'ssl'])\n"
     ]
    }
   ],
   "source": [
    "ignored_logs = [ # some of the logs contain information about the zeek session and are not useful for the analysis\n",
    "    \"loaded_scripts.log\",\n",
    "    \"capture_loss.log\",\n",
    "    \"stats.log\",\n",
    "    \"packet_filter.log\",\n",
    "    \"x509.log\",\n",
    "]\n",
    "zeek_logs = loading.load_all_zeek_logs(\"../stratosphere-work-challenge-v1/zeek\",ignored_logs) # change this to the path of the zeek logs\n",
    "\n",
    "print(zeek_logs.keys())\n",
    "cleaned_logs = {} #where cleaned and aggregated data will be stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etracting info from individual logs\n",
    "We select only a small number of important columns (based on prior knowledge AND inspecting the logs themselves. Some columns would be selected, but they have only one unique value (for example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  uid  http_count  ssl_count  avg_seen_bytes_files\n",
      "0  CYaRbd1LgVHyMi0os7           0          2                1392.0\n",
      "1  CE2v1V1PiJwfenwq22           0          2                1392.0\n",
      "2   CnCVN6i60NAbKmFxl           0          2                1392.0\n",
      "3   CXVBUUhqkJEhiN2s6           0          2                1424.0\n",
      "4  CriN9h1d6hCNTMi3P6           1          0                  22.0\n"
     ]
    }
   ],
   "source": [
    "# files log\n",
    "# multiple fuids for one uid. cannot connect 1:1, I want some kind of aggregation\n",
    "files_df = zeek_logs[\"files\"]\n",
    "http_count = []\n",
    "ssl_count = []\n",
    "avg_seen_bytes = []\n",
    "\n",
    "# for each uid, count the number of http and ssl files and calculate the average seen_bytes\n",
    "for uid in files_df['conn_uids'].unique():\n",
    "    http_files = files_df[(files_df['conn_uids'] == uid) & (files_df['source'] == 'HTTP')]\n",
    "    ssl_files = files_df[(files_df['conn_uids'] == uid) & (files_df['source'] == 'SSL')]\n",
    "    \n",
    "    http_count.append(len(http_files))\n",
    "    ssl_count.append(len(ssl_files))\n",
    "    avg_seen_bytes.append(files_df[files_df['conn_uids'] == uid]['seen_bytes'].mean())\n",
    "\n",
    "aggregated_file_df = pd.DataFrame({\n",
    "    'uid': files_df['conn_uids'].unique(),\n",
    "    'http_count': http_count,\n",
    "    'ssl_count': ssl_count,\n",
    "    'avg_seen_bytes_files': avg_seen_bytes\n",
    "})\n",
    "\n",
    "print(aggregated_file_df.head())\n",
    "cleaned_logs[\"files\"] = aggregated_file_df # no missing values and no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   uid  version resumed next_protocol established  \\\n",
      "0    CgnCcKyQn6Fuvtaaa  unknown       T       missing           T   \n",
      "1    CyaZhP168fSnjEW4i  unknown       T       missing           T   \n",
      "2   C1joeV1VXsHCLjiswg  unknown       T       missing           T   \n",
      "3    CplZRBVCFLcGq4Dbl  unknown       T       missing           T   \n",
      "4    C05gpYcCZjI5tZ4m5  unknown       T       missing           T   \n",
      "..                 ...      ...     ...           ...         ...   \n",
      "73  CoqeK14slRRWT3jSG3   TLSv13       F       missing           T   \n",
      "74   Cmz3aX3s2GpoaPURW   TLSv13       T       missing           T   \n",
      "75  Ci1SH62mLUPecPJx5d   TLSv12       T      http/1.1           T   \n",
      "76   CupYkjJ1BXaXaZk94   TLSv13       T       missing           T   \n",
      "77  CokYEW2q0cjgDIE5Gi   TLSv12       T            h2           T   \n",
      "\n",
      "   validation_status  \n",
      "0            missing  \n",
      "1            missing  \n",
      "2            missing  \n",
      "3            missing  \n",
      "4            missing  \n",
      "..               ...  \n",
      "73           missing  \n",
      "74           missing  \n",
      "75           missing  \n",
      "76           missing  \n",
      "77           missing  \n",
      "\n",
      "[78 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_532488/1487858966.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_ssl_df[\"version\"] = versions_col\n",
      "/tmp/ipykernel_532488/1487858966.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_ssl_df[\"next_protocol\"] = next_protocol_col\n",
      "/tmp/ipykernel_532488/1487858966.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_ssl_df[\"validation_status\"] = validation_status_col\n"
     ]
    }
   ],
   "source": [
    "# ssl contains info about https (but in conn, there already is tcp with ssl tags)\n",
    "ssl = zeek_logs[\"ssl\"]\n",
    "aggregated_ssl_df = ssl[\n",
    "    [\"uid\", \"version\", \"resumed\", \"next_protocol\", \"established\", \"validation_status\"]\n",
    "]\n",
    "\n",
    "# Replace missing values and specific string in 'version' column\n",
    "versions_col = aggregated_ssl_df.copy()[\"version\"]\n",
    "versions_col = versions_col.cat.rename_categories({\"unknown-64282\": \"unknown\"})\n",
    "versions_col = versions_col.cat.add_categories(\"missing\")\n",
    "versions_col = versions_col.replace(\"-\", None).fillna(\"missing\")\n",
    "aggregated_ssl_df[\"version\"] = versions_col\n",
    "\n",
    "# fill in next protocol column (missing if not specififed)\n",
    "next_protocol_col = aggregated_ssl_df.copy()[\"next_protocol\"]\n",
    "next_protocol_col = next_protocol_col.cat.add_categories(\"missing\")\n",
    "next_protocol_col = next_protocol_col.fillna(\"missing\")\n",
    "aggregated_ssl_df[\"next_protocol\"] = next_protocol_col\n",
    "\n",
    "# fill in validation status column (missing if not specified)\n",
    "validation_status_col = aggregated_ssl_df.copy()[\"validation_status\"]\n",
    "validation_status_col = validation_status_col.cat.add_categories(\"missing\")\n",
    "validation_status_col = validation_status_col.fillna(\"missing\")\n",
    "aggregated_ssl_df[\"validation_status\"] = validation_status_col\n",
    "\n",
    "cleaned_logs[\"ssl\"] = aggregated_ssl_df\n",
    "print(aggregated_ssl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   uid method  status_code  request_body_len  \\\n",
      "0   CBPoVA3CEo9RnTQNDj    GET          204                 0   \n",
      "1   CriN9h1d6hCNTMi3P6    GET          200                 0   \n",
      "2   CKwfPy2GoRYlUeD35i    GET          200                 0   \n",
      "3   CKwfPy2GoRYlUeD35i    GET          200                 0   \n",
      "4   CKwfPy2GoRYlUeD35i    GET          200                 0   \n",
      "5   Cpl4DO1V4VtiWE0oBj    GET          200                 0   \n",
      "6   Cpl4DO1V4VtiWE0oBj    GET          200                 0   \n",
      "7   Cpl4DO1V4VtiWE0oBj    GET          200                 0   \n",
      "8    C0Uxcf30mJ2Lq5wbX    GET          101                 0   \n",
      "9   CNPVre20YbYQSMf7Ke    GET          101                 0   \n",
      "10  Ctq0ZM2pGM8l5HdQS6    GET          200                 0   \n",
      "11  CtCYjh1iQvaBV7zgZ1    GET          200                 0   \n",
      "12  CnJ79x4juoPxV9NMW8    GET          200                 0   \n",
      "13  CUhwFI2YjMkHyGju97    GET          200                 0   \n",
      "14  CIrUMN3KNrQM1GRnU7    GET          200                 0   \n",
      "15  Ctq0ZM2pGM8l5HdQS6    GET          200                 0   \n",
      "16  CIrUMN3KNrQM1GRnU7   POST          200                 4   \n",
      "17  CUhwFI2YjMkHyGju97    GET          200                 0   \n",
      "18  Cde9is2KxtOFabBbzl    GET          101                 0   \n",
      "19  C9x7S83q8VCbuVz4wl    GET          200                 0   \n",
      "20  C9x7S83q8VCbuVz4wl    GET          200                 0   \n",
      "21  C9x7S83q8VCbuVz4wl    GET          200                 0   \n",
      "22  CUhwFI2YjMkHyGju97    GET            0                 0   \n",
      "23  CIrUMN3KNrQM1GRnU7   POST            0                 4   \n",
      "24  CYLKkn2Rnl8tV76MRi    GET          101                 0   \n",
      "\n",
      "    response_body_len  \n",
      "0                   0  \n",
      "1                  22  \n",
      "2                 101  \n",
      "3                   5  \n",
      "4                   4  \n",
      "5                 101  \n",
      "6                   5  \n",
      "7                   4  \n",
      "8                   0  \n",
      "9                   0  \n",
      "10                101  \n",
      "11                101  \n",
      "12                101  \n",
      "13                101  \n",
      "14                101  \n",
      "15                  5  \n",
      "16                  2  \n",
      "17                  4  \n",
      "18                  0  \n",
      "19                101  \n",
      "20                  5  \n",
      "21                  4  \n",
      "22                  0  \n",
      "23                  0  \n",
      "24                  0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_532488/2679241931.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_http_df['status_code'] = aggregated_http_df['status_code'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "#http log \n",
    "http = zeek_logs[\"http\"]\n",
    "aggregated_http_df = http[\n",
    "    [\"uid\",\"method\", \"status_code\", \"request_body_len\", \"response_body_len\"]]\n",
    "aggregated_http_df['status_code'] = aggregated_http_df['status_code'].fillna(0)\n",
    "\n",
    "cleaned_logs[\"http\"] = aggregated_http_df\n",
    "print(aggregated_http_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   uid qtype_name rcode_name  dns_answer_count  avg_TTL  TLD  \\\n",
      "0   CMLZbu3FDJYoZwa27k          A    NOERROR                 2    116.5  com   \n",
      "1   CgRCjV3z8dKmNVIvhb          A    NOERROR                 2   1800.5  com   \n",
      "2   CgFfWv3PUApAZUINNf          A    NOERROR                 1     59.0  com   \n",
      "3   CIkFu02IznJPZcp1El          A    NOERROR                 2   1814.5  com   \n",
      "4   Cylq6E2mc9lVLjs8ua          A    NOERROR                 2    636.5  com   \n",
      "..                 ...        ...        ...               ...      ...  ...   \n",
      "59  Cyw1E94W8FfeSPczJk          A    NOERROR                 1    299.0  com   \n",
      "60  C0jMGu1frh9ei1Wmqc          A    NOERROR                 1    247.0  com   \n",
      "61  COs0MF2BvtxqwwWZBl          A    NOERROR                 1    248.0  com   \n",
      "62  CovR8F24TUZtn0RYGj          A    NOERROR                 1    576.0   co   \n",
      "63   Cj4sBHa0s2uYpSCc3          A    NOERROR                 5   1793.0  com   \n",
      "\n",
      "             SLD  query_length  \n",
      "0             fb             3  \n",
      "1      instagram             3  \n",
      "2   cdninstagram             3  \n",
      "3      instagram             3  \n",
      "4       facebook             3  \n",
      "..           ...           ...  \n",
      "59    googleapis             3  \n",
      "60    googleapis             3  \n",
      "61    googleapis             3  \n",
      "62             t             4  \n",
      "63       twitter             3  \n",
      "\n",
      "[64 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_532488/2661032767.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_dns_df['dns_answer_count'] = dns_answer_count # number of dns answers\n",
      "/tmp/ipykernel_532488/2661032767.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_dns_df['avg_TTL'] = dns['TTLs'].apply(lambda x: np.mean([float(ttl) for ttl in x.split(',')]) if pd.notna(x) else 0)\n",
      "/tmp/ipykernel_532488/2661032767.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_dns_df[\"TLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-1])\n",
      "/tmp/ipykernel_532488/2661032767.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_dns_df[\"SLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-2] if len(x.split(\".\")) > 1 else \"\")\n",
      "/tmp/ipykernel_532488/2661032767.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_dns_df[\"query_length\"] = dns[\"query\"].apply(lambda x: len(x.split(\".\")))\n"
     ]
    }
   ],
   "source": [
    "dns = zeek_logs[\"dns\"]\n",
    "aggregated_dns_df = dns[[\"uid\", \"qtype_name\", \"rcode_name\"]]\n",
    "dns_answer_count = dns['answers'].apply(lambda x: len(x.split(',')) if pd.notna(x) else 0)\n",
    "\n",
    "# add new columns to the aggregated dns dataframe\n",
    "# calculate average ttl of answer packet (can mean very fast name change?)\n",
    "aggregated_dns_df['dns_answer_count'] = dns_answer_count # number of dns answers\n",
    "aggregated_dns_df['avg_TTL'] = dns['TTLs'].apply(lambda x: np.mean([float(ttl) for ttl in x.split(',')]) if pd.notna(x) else 0)\n",
    "\n",
    "\n",
    "# extract TLD (instead of the full query)\n",
    "aggregated_dns_df[\"TLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-1])\n",
    "# extract SLD (google in google.com ...)\n",
    "aggregated_dns_df[\"SLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-2] if len(x.split(\".\")) > 1 else \"\")\n",
    "# number of subdomains in the query (longer can be used for obfuscation)\n",
    "aggregated_dns_df[\"query_length\"] = dns[\"query\"].apply(lambda x: len(x.split(\".\")))\n",
    "\n",
    "print(aggregated_dns_df)\n",
    "cleaned_logs[\"dns\"] = aggregated_dns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p',\n",
      "       'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state',\n",
      "       'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts',\n",
      "       'resp_ip_bytes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "conn = zeek_logs[\"conn\"]\n",
    "# here we only drop some of the columns, most are useful and we need stuff liek IPs..)\n",
    "conn.drop(columns=[\"local_orig\", \"local_resp\", \"tunnel_parents\",\"proto\",\"log_type\"], inplace=True)\n",
    "merged_df = loading.convert_ip_addresses(conn, [\"id.orig_h\", \"id.resp_h\"])\n",
    "\n",
    "print(conn.columns)\n",
    "cleaned_logs[\"conn\"] = conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dns - qtype_name: 1 unique value\n",
      "dns - rcode_name: 1 unique value\n",
      "conn - missed_bytes: 1 unique value\n",
      "Merging files...\n",
      "Merging ssl...\n",
      "Merging http...\n",
      "Merging dns...\n",
      "flows in merged log file:  234\n",
      "columns:  32 Index(['ts', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'service',\n",
      "       'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'history',\n",
      "       'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes',\n",
      "       'http_count', 'ssl_count', 'avg_seen_bytes_files', 'version', 'resumed',\n",
      "       'next_protocol', 'established', 'validation_status', 'method',\n",
      "       'status_code', 'request_body_len', 'response_body_len',\n",
      "       'dns_answer_count', 'avg_TTL', 'TLD', 'SLD', 'query_length'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_532488/1113695692.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  log_df.drop(columns=[col], inplace=True)\n",
      "/tmp/ipykernel_532488/1113695692.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  log_df.drop(columns=[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# still, throw away columns with only one unique value\n",
    "for log_name, log_df in cleaned_logs.items():\n",
    "    for col in log_df.columns:\n",
    "        if log_df[col].nunique() == 1:\n",
    "            print(f\"{log_name} - {col}: {log_df[col].nunique()} unique value\")\n",
    "            log_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# finally merge logs\n",
    "merged_df = loading.merge_logs(\n",
    "    cleaned_logs, primary_log=\"conn\"\n",
    ")  \n",
    "merged_df.drop(columns=[\"uid\"], inplace=True)\n",
    "\n",
    "# results?\n",
    "print(\"flows in merged log file: \", len(merged_df))\n",
    "print(\"columns: \", len(merged_df.columns), merged_df.columns)\n",
    "\n",
    "#save for inspection?\n",
    "merged_df.to_csv(\"merged_zeek_logs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further clean-up the data \n",
    "Label the data and drop the time columns\n",
    "\n",
    "Fill in the missing values (some columns are filled just for some data).\n",
    "\n",
    "Remove columns which are obsolete (have only 1 value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate time of the start w.r.t. the first log entry and label the data as benign if the time is less than 4 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"calculate time of the start w.r.t. the first log entry and label the data as benign if the time is less than 4 minutes\")# based on task description\n",
    "merged_df['time_from_beginning'] = (merged_df['ts'] - merged_df['ts'].min()).dt.total_seconds()\n",
    "merged_df['label'] = np.where(merged_df['time_from_beginning'] < 4 * 60, 'benign', 'unknown')\n",
    "merged_df.drop(columns=[\"ts\", \"time_from_beginning\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert T/F columns to 1/0 and make them numerical\n",
    "tf_columns = merged_df.select_dtypes(include=['object']).columns\n",
    "for col in tf_columns:\n",
    "    if merged_df[col].isin(['T', 'F']).all():\n",
    "        merged_df[col] = merged_df[col].map({'T': 1, 'F': 0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column types: \n",
      "id.orig_h category 3\n",
      "id.orig_p UInt16 170\n",
      "id.resp_h category 41\n",
      "id.resp_p UInt16 8\n",
      "service category 3\n",
      "duration timedelta64[ns] 189\n",
      "orig_bytes UInt64 116\n",
      "resp_bytes UInt64 131\n",
      "conn_state category 8\n",
      "history category 46\n",
      "orig_pkts UInt64 50\n",
      "orig_ip_bytes UInt64 125\n",
      "resp_pkts UInt64 46\n",
      "resp_ip_bytes UInt64 133\n",
      "http_count float64 5\n",
      "ssl_count float64 3\n",
      "avg_seen_bytes_files float64 18\n",
      "version category 4\n",
      "resumed category 2\n",
      "next_protocol category 3\n",
      "established category 2\n",
      "validation_status category 2\n",
      "method category 2\n",
      "status_code UInt64 4\n",
      "request_body_len UInt64 2\n",
      "response_body_len UInt64 6\n",
      "dns_answer_count float64 6\n",
      "avg_TTL float64 57\n",
      "TLD object 4\n",
      "SLD object 17\n",
      "query_length float64 4\n",
      "label object 2\n",
      "   id.orig_h  id.orig_p  id.resp_h  id.resp_p service               duration  \\\n",
      "0  168296565       1210  134744072         53     dns 0 days 00:00:00.010542   \n",
      "1  168296565      43814  134744072         53     dns 0 days 00:00:00.010908   \n",
      "2  168296565      51631  134744072         53     dns 0 days 00:00:00.010734   \n",
      "3  168296565      65449  134744072         53     dns 0 days 00:00:00.010405   \n",
      "4  168296565      63247  134744072         53     dns 0 days 00:00:00.009332   \n",
      "\n",
      "   orig_bytes  resp_bytes conn_state history  ...  method  status_code  \\\n",
      "0          31          80         SF      Dd  ...     NaN         <NA>   \n",
      "1          33          87         SF      Dd  ...     NaN         <NA>   \n",
      "2          50          66         SF      Dd  ...     NaN         <NA>   \n",
      "3          37          91         SF      Dd  ...     NaN         <NA>   \n",
      "4          40          80         SF      Dd  ...     NaN         <NA>   \n",
      "\n",
      "   request_body_len  response_body_len  dns_answer_count  avg_TTL  TLD  \\\n",
      "0              <NA>               <NA>               2.0    116.5  com   \n",
      "1              <NA>               <NA>               2.0   1800.5  com   \n",
      "2              <NA>               <NA>               1.0     59.0  com   \n",
      "3              <NA>               <NA>               2.0   1814.5  com   \n",
      "4              <NA>               <NA>               2.0    636.5  com   \n",
      "\n",
      "            SLD query_length   label  \n",
      "0            fb          3.0  benign  \n",
      "1     instagram          3.0  benign  \n",
      "2  cdninstagram          3.0  benign  \n",
      "3     instagram          3.0  benign  \n",
      "4      facebook          3.0  benign  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"column types: \")\n",
    "for col in merged_df.columns:\n",
    "    print(col, merged_df[col].dtype, merged_df[col].nunique())\n",
    "merged_df.describe(include=\"all\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# categores to dummies\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# integers, floats\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# missing values everywhere?\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# see the jupyter for help?\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of columns in cleaned_df:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cleaned_df\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m      7\u001b[0m cleaned_df\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_df' is not defined"
     ]
    }
   ],
   "source": [
    "# categores to dummies\n",
    "# integers, floats\n",
    "# missing values everywhere?\n",
    "# see the jupyter for help?\n",
    "\n",
    "print(\"Number of columns in cleaned_df:\", len(cleaned_df.columns))\n",
    "cleaned_df.describe(include=\"all\")\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection, normalisation and dimensionality reduction\n",
    "Next steps in the pipeline: \n",
    "\n",
    "cluster the features or calculate colinearity/covariance (for feature selection)\n",
    "\n",
    "select normalisation, visualise the data, reduction (PCA?)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
