{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipaddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load selected logs for processing\n",
    "Some logs are not used, because they describe the zeek session and not the data.\n",
    "\n",
    "We decided to not use x509.log, since the data pertains to a very small portion of flows and most columns dont seem that important (type of eliptic curve? etc.). This log captures details on certificates exchanged during certain TLS negotiations. The dataset is very small and I wanted to prioritize hand-picked high-impact features, so the ML models used later converge well and dont struggle with excess dimensionality. If I had much larger dataset, I would more carefully consider every data for importance or try using different feature selection techniques to reduce the number columns.\n",
    "https://docs.zeek.org/en/master/logs/x509.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conn.log...\n",
      "Loading dns.log...\n",
      "Loading files.log...\n",
      "Loading http.log...\n",
      "Loading ssl.log...\n",
      "\n",
      "dict_keys(['conn', 'dns', 'files', 'http', 'ssl'])\n"
     ]
    }
   ],
   "source": [
    "ignored_logs = [ # some of the logs contain information about the zeek session and are not useful for the analysis\n",
    "    \"loaded_scripts.log\",\n",
    "    \"capture_loss.log\",\n",
    "    \"stats.log\",\n",
    "    \"packet_filter.log\",\n",
    "    \"x509.log\",\n",
    "]\n",
    "zeek_logs = loading.load_all_zeek_logs(\"../stratosphere-work-challenge-v1/zeek\",ignored_logs) # change this to the path of the zeek logs\n",
    "\n",
    "print(zeek_logs.keys())\n",
    "cleaned_logs = {} #where cleaned and aggregated data will be stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting info from individual logs\n",
    "We select only a small number of important columns (based on prior knowledge AND inspecting the logs themselves). Some columns would be selected, but they have only one unique value in the respective log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  uid  http_count  ssl_count  avg_seen_bytes_files\n",
      "0  CYaRbd1LgVHyMi0os7           0          2                1392.0\n",
      "1  CE2v1V1PiJwfenwq22           0          2                1392.0\n",
      "2   CnCVN6i60NAbKmFxl           0          2                1392.0\n",
      "3   CXVBUUhqkJEhiN2s6           0          2                1424.0\n",
      "4  CriN9h1d6hCNTMi3P6           1          0                  22.0\n"
     ]
    }
   ],
   "source": [
    "# files log\n",
    "# multiple fuids for one uid. cannot connect 1:1, I want some kind of aggregation\n",
    "files_df = zeek_logs[\"files\"]\n",
    "http_count = []\n",
    "ssl_count = []\n",
    "avg_seen_bytes = []\n",
    "\n",
    "# for each uid, count the number of http and ssl files and calculate the average seen_bytes\n",
    "for uid in files_df['conn_uids'].unique():\n",
    "    http_files = files_df[(files_df['conn_uids'] == uid) & (files_df['source'] == 'HTTP')]\n",
    "    ssl_files = files_df[(files_df['conn_uids'] == uid) & (files_df['source'] == 'SSL')]\n",
    "    \n",
    "    http_count.append(len(http_files))\n",
    "    ssl_count.append(len(ssl_files))\n",
    "    avg_seen_bytes.append(files_df[files_df['conn_uids'] == uid]['seen_bytes'].mean())\n",
    "\n",
    "aggregated_file_df = pd.DataFrame({\n",
    "    'uid': files_df['conn_uids'].unique(),\n",
    "    'http_count': http_count,\n",
    "    'ssl_count': ssl_count,\n",
    "    'avg_seen_bytes_files': avg_seen_bytes\n",
    "})\n",
    "\n",
    "print(aggregated_file_df.head())\n",
    "cleaned_logs[\"files\"] = aggregated_file_df # no missing values and no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   uid resumed established validation_status\n",
      "0    CgnCcKyQn6Fuvtaaa       T           T      not_provided\n",
      "1    CyaZhP168fSnjEW4i       T           T      not_provided\n",
      "2   C1joeV1VXsHCLjiswg       T           T      not_provided\n",
      "3    CplZRBVCFLcGq4Dbl       T           T      not_provided\n",
      "4    C05gpYcCZjI5tZ4m5       T           T      not_provided\n",
      "..                 ...     ...         ...               ...\n",
      "73  CoqeK14slRRWT3jSG3       F           T      not_provided\n",
      "74   Cmz3aX3s2GpoaPURW       T           T      not_provided\n",
      "75  Ci1SH62mLUPecPJx5d       T           T      not_provided\n",
      "76   CupYkjJ1BXaXaZk94       T           T      not_provided\n",
      "77  CokYEW2q0cjgDIE5Gi       T           T      not_provided\n",
      "\n",
      "[78 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265054/2156740250.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_ssl_df['validation_status'] = aggregated_ssl_df['validation_status'].cat.add_categories(\"not_provided\")\n",
      "/tmp/ipykernel_265054/2156740250.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_ssl_df['validation_status'] = aggregated_ssl_df['validation_status'].fillna(\"not_provided\")\n"
     ]
    }
   ],
   "source": [
    "# ssl contains info about https (but in conn, there already is tcp with ssl tags)\n",
    "ssl = zeek_logs[\"ssl\"]\n",
    "aggregated_ssl_df = ssl[\n",
    "    [\"uid\", \"resumed\", \"established\", \"validation_status\"]\n",
    "]\n",
    "\n",
    "aggregated_ssl_df['validation_status'] = aggregated_ssl_df['validation_status'].cat.add_categories(\"not_provided\")\n",
    "aggregated_ssl_df['validation_status'] = aggregated_ssl_df['validation_status'].fillna(\"not_provided\")\n",
    "\n",
    "cleaned_logs[\"ssl\"] = aggregated_ssl_df\n",
    "print(aggregated_ssl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   uid method status_code  request_body_len  response_body_len\n",
      "0   CBPoVA3CEo9RnTQNDj    GET         204                 0                  0\n",
      "1   CriN9h1d6hCNTMi3P6    GET         200                 0                 22\n",
      "2   CKwfPy2GoRYlUeD35i    GET         200                 0                101\n",
      "3   CKwfPy2GoRYlUeD35i    GET         200                 0                  5\n",
      "4   CKwfPy2GoRYlUeD35i    GET         200                 0                  4\n",
      "5   Cpl4DO1V4VtiWE0oBj    GET         200                 0                101\n",
      "6   Cpl4DO1V4VtiWE0oBj    GET         200                 0                  5\n",
      "7   Cpl4DO1V4VtiWE0oBj    GET         200                 0                  4\n",
      "8    C0Uxcf30mJ2Lq5wbX    GET         101                 0                  0\n",
      "9   CNPVre20YbYQSMf7Ke    GET         101                 0                  0\n",
      "10  Ctq0ZM2pGM8l5HdQS6    GET         200                 0                101\n",
      "11  CtCYjh1iQvaBV7zgZ1    GET         200                 0                101\n",
      "12  CnJ79x4juoPxV9NMW8    GET         200                 0                101\n",
      "13  CUhwFI2YjMkHyGju97    GET         200                 0                101\n",
      "14  CIrUMN3KNrQM1GRnU7    GET         200                 0                101\n",
      "15  Ctq0ZM2pGM8l5HdQS6    GET         200                 0                  5\n",
      "16  CIrUMN3KNrQM1GRnU7   POST         200                 4                  2\n",
      "17  CUhwFI2YjMkHyGju97    GET         200                 0                  4\n",
      "18  Cde9is2KxtOFabBbzl    GET         101                 0                  0\n",
      "19  C9x7S83q8VCbuVz4wl    GET         200                 0                101\n",
      "20  C9x7S83q8VCbuVz4wl    GET         200                 0                  5\n",
      "21  C9x7S83q8VCbuVz4wl    GET         200                 0                  4\n",
      "22  CUhwFI2YjMkHyGju97    GET           0                 0                  0\n",
      "23  CIrUMN3KNrQM1GRnU7   POST           0                 4                  0\n",
      "24  CYLKkn2Rnl8tV76MRi    GET         101                 0                  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265054/4248506904.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  aggregated_http_df['status_code'] = aggregated_http_df['status_code'].fillna(0).astype('category')\n"
     ]
    }
   ],
   "source": [
    "# http log \n",
    "http = zeek_logs[\"http\"]\n",
    "aggregated_http_df = http[\n",
    "    [\"uid\",\"method\", \"status_code\", \"request_body_len\", \"response_body_len\"]]\n",
    "aggregated_http_df['status_code'] = aggregated_http_df['status_code'].fillna(0).astype('category')\n",
    "\n",
    "cleaned_logs[\"http\"] = aggregated_http_df\n",
    "print(aggregated_http_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   uid qtype_name rcode_name  dns_answer_count  avg_TTL  TLD  \\\n",
      "0   CMLZbu3FDJYoZwa27k          A    NOERROR                 2    116.5  com   \n",
      "1   CgRCjV3z8dKmNVIvhb          A    NOERROR                 2   1800.5  com   \n",
      "2   CgFfWv3PUApAZUINNf          A    NOERROR                 1     59.0  com   \n",
      "3   CIkFu02IznJPZcp1El          A    NOERROR                 2   1814.5  com   \n",
      "4   Cylq6E2mc9lVLjs8ua          A    NOERROR                 2    636.5  com   \n",
      "..                 ...        ...        ...               ...      ...  ...   \n",
      "59  Cyw1E94W8FfeSPczJk          A    NOERROR                 1    299.0  com   \n",
      "60  C0jMGu1frh9ei1Wmqc          A    NOERROR                 1    247.0  com   \n",
      "61  COs0MF2BvtxqwwWZBl          A    NOERROR                 1    248.0  com   \n",
      "62  CovR8F24TUZtn0RYGj          A    NOERROR                 1    576.0   co   \n",
      "63   Cj4sBHa0s2uYpSCc3          A    NOERROR                 5   1793.0  com   \n",
      "\n",
      "    SLD_freq  query_length  \n",
      "0          1             3  \n",
      "1          6             3  \n",
      "2          1             3  \n",
      "3          6             3  \n",
      "4         15             3  \n",
      "..       ...           ...  \n",
      "59        10             3  \n",
      "60        10             3  \n",
      "61        10             3  \n",
      "62         4             4  \n",
      "63         7             3  \n",
      "\n",
      "[64 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265054/2788785345.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df['dns_answer_count'] = dns_answer_count # number of dns answers\n",
      "/tmp/ipykernel_265054/2788785345.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df['avg_TTL'] = dns['TTLs'].apply(lambda x: np.mean([float(ttl) for ttl in x.split(',')]) if pd.notna(x) else 0)\n",
      "/tmp/ipykernel_265054/2788785345.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df[\"TLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-1])\n",
      "/tmp/ipykernel_265054/2788785345.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df[\"TLD\"] = dns_df[\"TLD\"].astype(\"category\")\n",
      "/tmp/ipykernel_265054/2788785345.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df[\"SLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-2] if len(x.split(\".\")) > 1 else \"\")\n",
      "/tmp/ipykernel_265054/2788785345.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df['SLD_freq'] = dns_df['SLD'].map(sld_counts)\n",
      "/tmp/ipykernel_265054/2788785345.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df.drop(columns=['SLD'],inplace=True)\n",
      "/tmp/ipykernel_265054/2788785345.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dns_df[\"query_length\"] = dns[\"query\"].apply(lambda x: len(x.split(\".\")))\n"
     ]
    }
   ],
   "source": [
    "#dns log\n",
    "dns = zeek_logs[\"dns\"]\n",
    "dns_df = dns[[\"uid\", \"qtype_name\", \"rcode_name\"]]\n",
    "dns_answer_count = dns['answers'].apply(lambda x: len(x.split(',')) if pd.notna(x) else 0).astype(int)\n",
    "dns_df['dns_answer_count'] = dns_answer_count # number of dns answers\n",
    "\n",
    "# calculate average ttl of answer packet (can mean very fast name change? fast-flux servers)\n",
    "dns_df['avg_TTL'] = dns['TTLs'].apply(lambda x: np.mean([float(ttl) for ttl in x.split(',')]) if pd.notna(x) else 0)\n",
    "\n",
    "\n",
    "# extract TLD (instead of the full query)\n",
    "dns_df[\"TLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-1])\n",
    "dns_df[\"TLD\"] = dns_df[\"TLD\"].astype(\"category\")\n",
    "\n",
    "# extract SLD (google in google.com ...)\n",
    "dns_df[\"SLD\"] = dns[\"query\"].apply(lambda x: x.split(\".\")[-2] if len(x.split(\".\")) > 1 else \"\")\n",
    "# Frequency encoding for SLD (many values for sld, I want at least their relative frequency)\n",
    "sld_counts = dns_df['SLD'].value_counts()\n",
    "dns_df['SLD_freq'] = dns_df['SLD'].map(sld_counts)\n",
    "dns_df.drop(columns=['SLD'],inplace=True)\n",
    "\n",
    "# number of subdomains in the query (longer can be used for obfuscation)\n",
    "dns_df[\"query_length\"] = dns[\"query\"].apply(lambda x: len(x.split(\".\")))\n",
    "\n",
    "print(dns_df)\n",
    "cleaned_logs[\"dns\"] = dns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p',\n",
      "       'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state',\n",
      "       'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts',\n",
      "       'resp_ip_bytes', 'is_icmp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "conn = zeek_logs[\"conn\"]\n",
    "# here we only drop some of the columns, most are useful and we need stuff like IPs..)\n",
    "conn.drop(columns=[\"local_orig\", \"local_resp\",\"history\", \"tunnel_parents\",\"log_type\"], inplace=True)\n",
    "\n",
    "# Create a new binary column 'is_icmp' based on the 'proto' column. udp and tcp have other information in \"service\"\n",
    "conn[\"is_icmp\"] = np.where(conn[\"proto\"] == \"icmp\", 1, 0)\n",
    "\n",
    "conn[\"id.orig_h\"] = conn[\"id.orig_h\"].apply(lambda x: int(ipaddress.ip_address(x)))\n",
    "conn[\"id.resp_h\"] = conn[\"id.resp_h\"].apply(lambda x: int(ipaddress.ip_address(x)))\n",
    "\n",
    "# Convert IP address columns to numerical\n",
    "conn[\"id.orig_h\"] = conn[\"id.orig_h\"].astype(np.int32)\n",
    "conn[\"id.resp_h\"] = conn[\"id.resp_h\"].astype(np.int32)\n",
    "\n",
    "conn.drop(columns=[\"proto\"], inplace=True)\n",
    "print(conn.columns)\n",
    "cleaned_logs[\"conn\"] = conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dns - qtype_name: 1 unique value\n",
      "dns - rcode_name: 1 unique value\n",
      "conn - missed_bytes: 1 unique value\n",
      "Merging files...\n",
      "Merging ssl...\n",
      "Merging http...\n",
      "Merging dns...\n",
      "flows in merged log file:  234\n",
      "columns:  30 Index(['ts', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'service',\n",
      "       'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'orig_pkts',\n",
      "       'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'is_icmp', 'http_count',\n",
      "       'ssl_count', 'avg_seen_bytes_files', 'resumed', 'established',\n",
      "       'validation_status', 'method', 'status_code', 'request_body_len',\n",
      "       'response_body_len', 'dns_answer_count', 'avg_TTL', 'TLD', 'SLD_freq',\n",
      "       'query_length'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265054/1816079443.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  log_df.drop(columns=[col], inplace=True)\n",
      "/tmp/ipykernel_265054/1816079443.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  log_df.drop(columns=[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# still, throw away columns with only one unique value from any log\n",
    "for log_name, log_df in cleaned_logs.items():\n",
    "    for col in log_df.columns:\n",
    "        if log_df[col].nunique() == 1:\n",
    "            print(f\"{log_name} - {col}: {log_df[col].nunique()} unique value\")\n",
    "            log_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# finally merge logs\n",
    "merged_df = loading.merge_logs(\n",
    "    cleaned_logs, primary_log=\"conn\"\n",
    ")  \n",
    "merged_df.drop(columns=[\"uid\"], inplace=True)\n",
    "\n",
    "# results?\n",
    "print(\"flows in merged log file: \", len(merged_df))\n",
    "print(\"columns: \", len(merged_df.columns), merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further clean-up the data \n",
    "#### Label the data and drop the time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate time of the start w.r.t. the first log entry and label the data as benign if the time is less than 4 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"calculate time of the start w.r.t. the first log entry and label the data as benign if the time is less than 4 minutes\")# based on task description\n",
    "merged_df['time_from_beginning'] = (merged_df['ts'] - merged_df['ts'].min()).dt.total_seconds()\n",
    "merged_df[\"label\"] = np.where(\n",
    "    merged_df[\"time_from_beginning\"] < 4 * 60, \"benign\", \"unknown\"\n",
    ")\n",
    "merged_df['label'] = merged_df['label'].astype('category')\n",
    "merged_df.drop(columns=[\"ts\", \"time_from_beginning\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the columns, their names and how many unique values do they have. we should have numerical columns, some categories and duration which is a time column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column types: \n",
      "id.orig_h int32 3\n",
      "id.orig_p UInt16 170\n",
      "id.resp_h int32 41\n",
      "id.resp_p UInt16 8\n",
      "service category 3\n",
      "duration timedelta64[ns] 189\n",
      "orig_bytes UInt64 116\n",
      "resp_bytes UInt64 131\n",
      "conn_state category 8\n",
      "orig_pkts UInt64 50\n",
      "orig_ip_bytes UInt64 125\n",
      "resp_pkts UInt64 46\n",
      "resp_ip_bytes UInt64 133\n",
      "is_icmp int64 2\n",
      "http_count float64 5\n",
      "ssl_count float64 3\n",
      "avg_seen_bytes_files float64 18\n",
      "resumed category 2\n",
      "established category 2\n",
      "validation_status category 2\n",
      "method category 2\n",
      "status_code category 4\n",
      "request_body_len UInt64 2\n",
      "response_body_len UInt64 6\n",
      "dns_answer_count float64 6\n",
      "avg_TTL float64 57\n",
      "TLD category 4\n",
      "SLD_freq float64 7\n",
      "query_length float64 4\n",
      "label category 2\n"
     ]
    }
   ],
   "source": [
    "print(\"column types: \")\n",
    "for col in merged_df.columns:\n",
    "    print(col, merged_df[col].dtype, merged_df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in cleaned_df: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id.orig_h</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_h</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>service</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>conn_state</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>...</th>\n",
       "      <th>method</th>\n",
       "      <th>status_code</th>\n",
       "      <th>request_body_len</th>\n",
       "      <th>response_body_len</th>\n",
       "      <th>dns_answer_count</th>\n",
       "      <th>avg_TTL</th>\n",
       "      <th>TLD</th>\n",
       "      <th>SLD_freq</th>\n",
       "      <th>query_length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.340000e+02</td>\n",
       "      <td>234.0</td>\n",
       "      <td>2.340000e+02</td>\n",
       "      <td>234.0</td>\n",
       "      <td>167</td>\n",
       "      <td>201</td>\n",
       "      <td>201.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>234</td>\n",
       "      <td>234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ssl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>SF</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>GET</td>\n",
       "      <td>200</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>149</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.867825e+08</td>\n",
       "      <td>42676.0</td>\n",
       "      <td>-5.680145e+08</td>\n",
       "      <td>3333.273504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:01:16.139389960</td>\n",
       "      <td>6151.945274</td>\n",
       "      <td>176449.074627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.837607</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32</td>\n",
       "      <td>34.72</td>\n",
       "      <td>2.203125</td>\n",
       "      <td>2813.152604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.593750</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.404771e+08</td>\n",
       "      <td>16161.352785</td>\n",
       "      <td>1.125869e+09</td>\n",
       "      <td>7954.089616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:02:53.423000225</td>\n",
       "      <td>49674.233238</td>\n",
       "      <td>1362814.758861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>491.278309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.10755</td>\n",
       "      <td>46.602504</td>\n",
       "      <td>1.335471</td>\n",
       "      <td>5715.046623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.904545</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.682964e+08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.826598e+09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:00:00.000043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.682966e+08</td>\n",
       "      <td>38682.0</td>\n",
       "      <td>-1.826598e+09</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:00:00.011053</td>\n",
       "      <td>38.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.682966e+08</td>\n",
       "      <td>47811.0</td>\n",
       "      <td>1.347441e+08</td>\n",
       "      <td>443.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:00:03.260685</td>\n",
       "      <td>517.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>927.583333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.682966e+08</td>\n",
       "      <td>52183.5</td>\n",
       "      <td>1.347441e+08</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:01:05.023765</td>\n",
       "      <td>1485.0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1815.375000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.249724e+09</td>\n",
       "      <td>65449.0</td>\n",
       "      <td>1.760832e+09</td>\n",
       "      <td>57736.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:30:08.665546</td>\n",
       "      <td>692835.0</td>\n",
       "      <td>18081893.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6927.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>21503.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id.orig_h     id.orig_p     id.resp_h    id.resp_p service  \\\n",
       "count   2.340000e+02         234.0  2.340000e+02        234.0     167   \n",
       "unique           NaN          <NA>           NaN         <NA>       3   \n",
       "top              NaN          <NA>           NaN         <NA>     ssl   \n",
       "freq             NaN          <NA>           NaN         <NA>      78   \n",
       "mean    1.867825e+08       42676.0 -5.680145e+08  3333.273504     NaN   \n",
       "std     1.404771e+08  16161.352785  1.125869e+09  7954.089616     NaN   \n",
       "min     1.682964e+08           3.0 -1.826598e+09          4.0     NaN   \n",
       "25%     1.682966e+08       38682.0 -1.826598e+09         53.0     NaN   \n",
       "50%     1.682966e+08       47811.0  1.347441e+08        443.0     NaN   \n",
       "75%     1.682966e+08       52183.5  1.347441e+08       8000.0     NaN   \n",
       "max     1.249724e+09       65449.0  1.760832e+09      57736.0     NaN   \n",
       "\n",
       "                         duration    orig_bytes      resp_bytes conn_state  \\\n",
       "count                         201         201.0           201.0        234   \n",
       "unique                        NaN          <NA>            <NA>          8   \n",
       "top                           NaN          <NA>            <NA>         SF   \n",
       "freq                          NaN          <NA>            <NA>        149   \n",
       "mean    0 days 00:01:16.139389960   6151.945274   176449.074627        NaN   \n",
       "std     0 days 00:02:53.423000225  49674.233238  1362814.758861        NaN   \n",
       "min        0 days 00:00:00.000043           0.0             0.0        NaN   \n",
       "25%        0 days 00:00:00.011053          38.0            66.0        NaN   \n",
       "50%        0 days 00:00:03.260685         517.0           494.0        NaN   \n",
       "75%        0 days 00:01:05.023765        1485.0          3418.0        NaN   \n",
       "max        0 days 00:30:08.665546      692835.0      18081893.0        NaN   \n",
       "\n",
       "         orig_pkts  ...  method  status_code  request_body_len  \\\n",
       "count        234.0  ...      25           25              25.0   \n",
       "unique        <NA>  ...       2            4              <NA>   \n",
       "top           <NA>  ...     GET          200              <NA>   \n",
       "freq          <NA>  ...      23           18              <NA>   \n",
       "mean     71.837607  ...     NaN          NaN              0.32   \n",
       "std     491.278309  ...     NaN          NaN           1.10755   \n",
       "min            0.0  ...     NaN          NaN               0.0   \n",
       "25%            1.0  ...     NaN          NaN               0.0   \n",
       "50%            3.0  ...     NaN          NaN               0.0   \n",
       "75%           15.0  ...     NaN          NaN               0.0   \n",
       "max         6927.0  ...     NaN          NaN               4.0   \n",
       "\n",
       "        response_body_len  dns_answer_count       avg_TTL  TLD   SLD_freq  \\\n",
       "count                25.0         64.000000     64.000000   64  64.000000   \n",
       "unique               <NA>               NaN           NaN    4        NaN   \n",
       "top                  <NA>               NaN           NaN  com        NaN   \n",
       "freq                 <NA>               NaN           NaN   54        NaN   \n",
       "mean                34.72          2.203125   2813.152604  NaN   7.593750   \n",
       "std             46.602504          1.335471   5715.046623  NaN   4.904545   \n",
       "min                   0.0          1.000000     17.000000  NaN   1.000000   \n",
       "25%                   0.0          1.000000    247.500000  NaN   4.000000   \n",
       "50%                   5.0          2.000000    927.583333  NaN   6.500000   \n",
       "75%                 101.0          3.000000   1815.375000  NaN  10.000000   \n",
       "max                 101.0          6.000000  21503.000000  NaN  15.000000   \n",
       "\n",
       "       query_length    label  \n",
       "count     64.000000      234  \n",
       "unique          NaN        2  \n",
       "top             NaN  unknown  \n",
       "freq            NaN      132  \n",
       "mean       3.250000      NaN  \n",
       "std        0.503953      NaN  \n",
       "min        2.000000      NaN  \n",
       "25%        3.000000      NaN  \n",
       "50%        3.000000      NaN  \n",
       "75%        3.250000      NaN  \n",
       "max        5.000000      NaN  \n",
       "\n",
       "[11 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of columns in cleaned_df:\", len(merged_df.columns))\n",
    "merged_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service', 'duration', 'orig_bytes', 'resp_bytes', 'http_count', 'ssl_count', 'avg_seen_bytes_files', 'resumed', 'established', 'validation_status', 'method', 'status_code', 'request_body_len', 'response_body_len', 'dns_answer_count', 'avg_TTL', 'TLD', 'SLD_freq', 'query_length']\n"
     ]
    }
   ],
   "source": [
    "#which columns do have empty data?\n",
    "print(merged_df.columns[merged_df.isna().any()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265054/869725158.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"orig_bytes\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"resp_bytes\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"duration\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0' has dtype incompatible with timedelta64[ns], please explicitly cast to a compatible dtype first.\n",
      "  merged_df[\"duration\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"http_count\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"ssl_count\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"avg_seen_bytes_files\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"request_body_len\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"response_body_len\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"dns_answer_count\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"avg_TTL\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"SLD_freq\"].fillna(0.0, inplace=True)\n",
      "/tmp/ipykernel_265054/869725158.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"query_length\"].fillna(0.0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "merged_df[\"orig_bytes\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"resp_bytes\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"duration\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"http_count\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"ssl_count\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"avg_seen_bytes_files\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"request_body_len\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"response_body_len\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"dns_answer_count\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"avg_TTL\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"SLD_freq\"].fillna(0.0, inplace=True)\n",
    "merged_df[\"query_length\"].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service', 'resumed', 'established', 'validation_status', 'method', 'status_code', 'TLD']\n",
      "Unique values in column service: ['dns', 'ssl', NaN, 'http']\n",
      "Categories (3, object): ['dns', 'http', 'ssl']\n",
      "Unique values in column resumed: [NaN, 'T', 'F']\n",
      "Categories (2, object): ['F', 'T']\n",
      "Unique values in column established: [NaN, 'T', 'F']\n",
      "Categories (2, object): ['F', 'T']\n",
      "Unique values in column validation_status: [NaN, 'not_provided', 'ok']\n",
      "Categories (2, object): ['ok', 'not_provided']\n",
      "Unique values in column method: [NaN, 'GET', 'POST']\n",
      "Categories (2, object): ['GET', 'POST']\n",
      "Unique values in column status_code: [NaN, 204, 200, 101, 0]\n",
      "Categories (4, UInt64): [0, 101, 200, 204]\n",
      "Unique values in column TLD: ['com', 'net', NaN, 'co', 'ms']\n",
      "Categories (4, object): ['co', 'com', 'ms', 'net']\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns[merged_df.isna().any()].tolist())\n",
    "for col in merged_df.columns[merged_df.isna().any()]:\n",
    "    print(f\"Unique values in column {col}: {merged_df[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265054/4286792628.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['service'].fillna(\"no_service\", inplace=True)\n",
      "/tmp/ipykernel_265054/4286792628.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['resumed'].fillna(\"missing\", inplace=True)\n",
      "/tmp/ipykernel_265054/4286792628.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['established'].fillna(\"missing\", inplace=True)\n",
      "/tmp/ipykernel_265054/4286792628.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['validation_status'].fillna(\"missing\", inplace=True)\n",
      "/tmp/ipykernel_265054/4286792628.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['method'].fillna(\"no_method\", inplace=True)\n",
      "/tmp/ipykernel_265054/4286792628.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['status_code'].fillna(\"missing\", inplace=True)\n",
      "/tmp/ipykernel_265054/4286792628.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['TLD'].fillna(\"missing\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# for all the categories, those are categories from respective logs, which just dont exist for all flows (like type of http request)\n",
    "# will fill missing valies for those, but are largely redundant, since if you dont have one, you dont have other from the same log. \n",
    "\n",
    "# service column\n",
    "merged_df['service'] = merged_df['service'].cat.add_categories(\"no_service\")\n",
    "merged_df['service'].fillna(\"no_service\", inplace=True)\n",
    "\n",
    "# resumed column\n",
    "merged_df['resumed'] = merged_df['resumed'].cat.add_categories(\"missing\")\n",
    "merged_df['resumed'].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# established column\n",
    "merged_df['established'] = merged_df['established'].cat.add_categories(\"missing\")\n",
    "merged_df['established'].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# validation_status column\n",
    "merged_df['validation_status'] = merged_df['validation_status'].cat.add_categories(\"missing\")\n",
    "merged_df['validation_status'].fillna(\"missing\", inplace=True)\n",
    "\n",
    "#method column\n",
    "merged_df['method'] = merged_df['method'].cat.add_categories(\"no_method\")\n",
    "merged_df['method'].fillna(\"no_method\", inplace=True)\n",
    "\n",
    "# status code column\n",
    "merged_df['status_code'] = merged_df['status_code'].cat.add_categories(\"missing\")\n",
    "merged_df['status_code'].fillna(\"missing\", inplace=True)\n",
    "\n",
    "#tld\n",
    "merged_df['TLD'] = merged_df['TLD'].cat.add_categories(\"missing\")\n",
    "merged_df['TLD'].fillna(\"missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy columns for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service: 4 unique values\n",
      "conn_state: 8 unique values\n",
      "resumed: 3 unique values\n",
      "established: 3 unique values\n",
      "validation_status: 3 unique values\n",
      "method: 3 unique values\n",
      "status_code: 5 unique values\n",
      "TLD: 5 unique values\n",
      "label: 2 unique values\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = merged_df.select_dtypes(include=['category']).columns\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(f\"{col}: {merged_df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after generating dummy variables: 48\n",
      "Columns: Index(['id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'duration',\n",
      "       'orig_bytes', 'resp_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts',\n",
      "       'resp_ip_bytes', 'is_icmp', 'http_count', 'ssl_count',\n",
      "       'avg_seen_bytes_files', 'request_body_len', 'response_body_len',\n",
      "       'dns_answer_count', 'avg_TTL', 'SLD_freq', 'query_length',\n",
      "       'service_http', 'service_ssl', 'service_no_service', 'conn_state_RSTO',\n",
      "       'conn_state_RSTR', 'conn_state_S0', 'conn_state_S1', 'conn_state_S3',\n",
      "       'conn_state_SF', 'conn_state_SH', 'resumed_T', 'resumed_missing',\n",
      "       'established_T', 'established_missing',\n",
      "       'validation_status_not_provided', 'validation_status_missing',\n",
      "       'method_POST', 'method_no_method', 'status_code_101', 'status_code_200',\n",
      "       'status_code_204', 'status_code_missing', 'TLD_com', 'TLD_ms',\n",
      "       'TLD_net', 'TLD_missing', 'label_unknown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy columns for all categorical columns\n",
    "merged_df = pd.get_dummies(merged_df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(\"Number of columns after generating dummy variables:\", len(merged_df.columns))\n",
    "print(\"Columns:\", merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 48\n",
      "Number of flows: 234\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns: {merged_df.shape[1]}\")\n",
    "print(f\"Number of flows: {merged_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for inspection?\n",
    "merged_df.to_csv(\"merged_zeek_logs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
